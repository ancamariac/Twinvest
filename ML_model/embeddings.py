import pandas as pd
import numpy as np
import tensorflow as tf
from transformers import BertTokenizer, TFBertModel

# BERT model transforma propozitiile intr un vector de numere

df = pd.read_csv("all-data.csv", encoding="ISO-8859-1")

#print(df.head(5))

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

model = TFBertModel.from_pretrained("bert-base-uncased")

# liste pentru noul dataset (x_data-embeddings, y_data-labels)
x_data = []
y_data = []
i = 0
for index, row in df.iterrows():
    # transform labelul intr-un numar
    if row[0] == 'neutral':
        y_data.append(1)
    elif row[0] == 'positive':
        y_data.append(2)
    elif row[0] == 'negative':
        y_data.append(0)
    else:
        continue

    # apelez modelul BERT pentru fiecare propozitie
    input_ids = tf.constant(tokenizer.encode(row[1], max_length=512, truncation=True))[None, :]
    outputs = model(input_ids)
    if i % 10 == 0:
        print(i)
    i += 1

    # folosesc doar ultimul strat returnat de modelul BERT
    x_data.append(np.array(outputs["pooler_output"]).reshape(768))
    if i > 4840:
        break

# salvam setul de date pe care il vom folosi la antrenarea modelului nostru
import pickle

with open('embeddings.pk', 'wb') as f:
    pickle.dump([x_data, y_data], f)


"""
model:
    strat dense input, activare ReLU
    strat overfitting
    straturi dense hidden (testam sa vedem cate facem si cat de mari sa fie fiecare), activare ReLU
    strat output, 3 neuroni, activare softmax

"""